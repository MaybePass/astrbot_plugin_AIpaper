import os
import json
import re
import asyncio
import aiohttp
from apscheduler.schedulers.asyncio import AsyncIOScheduler

# å¯¼å…¥ fitz å’Œ feedparserï¼Œä½†ä¸åœ¨ä¸»çº¿ç¨‹ç›´æ¥è°ƒç”¨è€—æ—¶æ“ä½œ
import feedparser
import fitz  # PyMuPDF

from astrbot.api import star, logger
from astrbot.api.event import AstrMessageEvent, MessageEventResult, filter, MessageChain
from astrbot.api.message_components import Plain, Image, Node, Nodes, BaseMessageComponent
from astrbot.api.star import StarTools
from astrbot.core.platform.sources.aiocqhttp.aiocqhttp_message_event import AiocqhttpMessageEvent
from astrbot.core.platform.sources.aiocqhttp.aiocqhttp_platform_adapter import AiocqhttpAdapter

class Main(star.Star):
    def __init__(self, context: star.Context, config: dict = None):
        super().__init__(context, config)
        if not config:
            config = {}
        self.config = config
        
        # 1. é…ç½®è¯»å–ä¸å¥å£®æ€§æ£€æŸ¥
        self.target_groups = self._parse_target_groups(self.config.get("target_groups"))
        self.push_time = self.config.get("push_time", "09:00")
        self.extra_message = self.config.get("extra_message", "")
        
        default_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI è®ºæ–‡è§£è¯»åŠ©æ‰‹ã€‚\n\n"
            "è®ºæ–‡æ ‡é¢˜: {title}\n"
            "ä½œè€…: {authors}\n"
            "æ‘˜è¦: {abstract}\n\n"
            "è®ºæ–‡å†…å®¹ç‰‡æ®µ:\n{full_text}\n\n"
            "è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ Markdown æ ¼å¼è¾“å‡ºï¼Œä¸è¦è¾“å‡ºå…¶ä»–å¯’æš„è¯­ï¼š\n\n"
            "## ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ç‚¹\n(ç®€è¦æ¦‚æ‹¬)\n\n"
            "## ğŸ“– è®ºæ–‡æ¦‚è¦\n(é€šä¿—è§£é‡Šè¿™ç¯‡è®ºæ–‡è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Œç”¨äº†ä»€ä¹ˆæ–¹æ³•)\n\n"
            "## ğŸ‘¥ ä½œè€…èƒŒæ™¯\n(æ ¹æ®ä½œè€…å§“åç®€è¦ä»‹ç»å…¶æ‰€å±æœºæ„æˆ–çŸ¥åä»£è¡¨ä½œï¼Œå¦‚æœæ— æ³•ç¡®å®šåˆ™ç•¥è¿‡)\n\n"
            "## ğŸ”¬ å…³é”®ç»“è®º\n(å®éªŒç»“æœæˆ–ç†è®ºè´¡çŒ®)"
        )
        self.prompt_template = self.config.get("prompt_template", default_prompt)

        # 2. æ•°æ®æŒä¹…åŒ–è§„èŒƒ
        self.data_dir = StarTools.get_data_dir("astrbot_plugin_daily_paper")
        self.history_file = os.path.join(self.data_dir, "history.json")
        self.temp_dir = os.path.join(self.data_dir, "temp")
        
        if not os.path.exists(self.temp_dir):
            os.makedirs(self.temp_dir)
            
        self.history = self._load_history()
        
        # 3. å®šæ—¶ä»»åŠ¡åˆå§‹åŒ–
        self.scheduler = AsyncIOScheduler()
        try:
            hour, minute = map(int, self.push_time.split(":"))
            self.scheduler.add_job(self.run_daily_push, 'cron', hour=hour, minute=minute)
            self.scheduler.start()
            logger.info(f"AIè®ºæ–‡æ¨é€å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨: {self.push_time}, ç›®æ ‡ç¾¤: {self.target_groups}")
        except Exception as e:
            logger.error(f"å®šæ—¶ä»»åŠ¡å¯åŠ¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¶é—´æ ¼å¼(HH:MM): {e}")

    # --- æ–°å¢ï¼šç”Ÿå‘½å‘¨æœŸç®¡ç† ---
    async def terminate(self):
        """æ’ä»¶å¸è½½/é‡è½½æ—¶è°ƒç”¨ï¼Œæ¸…ç†èµ„æº"""
        logger.info("æ­£åœ¨åœæ­¢è®ºæ–‡æ¨é€æ’ä»¶å®šæ—¶ä»»åŠ¡...")
        if self.scheduler.running:
            self.scheduler.shutdown(wait=False)
    # ------------------------

    def _parse_target_groups(self, config_val):
        if not config_val:
            return []
        if isinstance(config_val, list):
            return [str(g).strip() for g in config_val]
        if isinstance(config_val, str):
            return [g.strip() for g in re.split(r'[,ï¼Œ]', config_val) if g.strip()]
        return []

    def _load_history(self):
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return []
        return []

    def _save_history(self):
        try:
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(self.history, f)
        except Exception as e:
            logger.error(f"ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")

    # --- å¼‚æ­¥/çº¿ç¨‹æ± åŒ…è£…å™¨ ---

    async def _parse_feed_in_thread(self, data):
        return await asyncio.to_thread(feedparser.parse, data)

    async def _process_pdf_in_thread(self, pdf_path, img_path):
        def _heavy_work():
            doc = None
            try:
                logger.debug(f"[PDFè§£æ] å¼€å§‹å¤„ç†: {pdf_path}")
                doc = fitz.open(pdf_path)
                text_content = ""
                # æå–å‰2é¡µæ–‡æœ¬
                for page_num, page in enumerate(doc[:2]): 
                    text_content += page.get_text()
                
                # ç”Ÿæˆç¬¬ä¸€é¡µé¢„è§ˆå›¾
                page = doc.load_page(0)
                pix = page.get_pixmap(dpi=150)
                pix.save(img_path)
                logger.info(f"[PDFè§£æ] é¢„è§ˆå›¾å·²ä¿å­˜è‡³: {img_path}")
                return text_content
            except Exception as e:
                logger.error(f"[PDFè§£æ] PyMuPDFå¤„ç†å¤±è´¥: {e}")
                return None
            finally:
                if doc:
                    doc.close()
        
        return await asyncio.to_thread(_heavy_work)

    # --- æ ¸å¿ƒé€»è¾‘ ---

    async def _call_arxiv_api(self, query_url):
        logger.debug(f"Requesting ArXiv: {query_url}")
        connector = aiohttp.TCPConnector(ssl=False)
        async with aiohttp.ClientSession(connector=connector) as session:
            try:
                async with session.get(query_url, timeout=30) as response:
                    if response.status != 200:
                        logger.error(f"ArXiv API error: {response.status}")
                        return None
                    data = await response.text()
                    return await self._parse_feed_in_thread(data)
            except Exception as e:
                logger.error(f"Network error fetching arxiv: {e}")
                return None

    async def fetch_latest_paper(self):
        url = "http://export.arxiv.org/api/query?search_query=cat:cs.AI+OR+cat:cs.CV+OR+cat:cs.CL&sortBy=submittedDate&sortOrder=descending&max_results=50"
        feed = await self._call_arxiv_api(url)
        
        if not feed or not feed.entries:
            return None

        for entry in feed.entries:
            paper_id = entry.id.split('/')[-1]
            if paper_id not in self.history:
                return self._parse_entry(entry, paper_id)
        
        logger.warning("æœ€è¿‘ 50 ç¯‡è®ºæ–‡éƒ½å·²æ¨é€è¿‡ã€‚")
        return None

    async def fetch_specific_paper(self, query: str):
        id_pattern = r"(\d{4}\.\d{4,5})"
        match = re.search(id_pattern, query)
        
        url = ""
        if match:
            paper_id = match.group(1)
            url = f"http://export.arxiv.org/api/query?id_list={paper_id}"
        else:
            safe_query = query.replace(" ", "+")
            url = f"http://export.arxiv.org/api/query?search_query=ti:{safe_query}&max_results=1"

        feed = await self._call_arxiv_api(url)
        if not feed or not feed.entries:
            return None
        
        entry = feed.entries[0]
        paper_id = entry.id.split('/')[-1]
        return self._parse_entry(entry, paper_id)

    def _parse_entry(self, entry, paper_id):
        return {
            "id": paper_id,
            "title": entry.title.replace('\n', ' '),
            "summary": entry.summary,
            "authors": [a.name for a in entry.authors],
            "link": entry.link,
            "pdf_link": entry.link.replace("abs", "pdf")
        }

    async def process_pdf(self, pdf_url, paper_id, max_retries=3):
        """ä¸‹è½½å¹¶å¤„ç† PDF"""
        pdf_path = os.path.join(self.temp_dir, f"{paper_id}.pdf")
        img_path = os.path.join(self.temp_dir, f"{paper_id}.png")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/pdf,text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Connection': 'keep-alive',
        }
        
        for attempt in range(max_retries):
            try:
                logger.info(f"[PDFä¸‹è½½] å°è¯• {attempt+1}/{max_retries}ï¼ŒID: {paper_id}")
                
                connector = aiohttp.TCPConnector(ssl=False)
                timeout = aiohttp.ClientTimeout(total=120, connect=30, sock_read=60)
                
                async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
                    async with session.get(
                        pdf_url,
                        headers=headers,
                        allow_redirects=True
                    ) as resp:
                        status = resp.status
                        content_type = resp.headers.get('Content-Type', '')
                        
                        logger.info(f"[PDFä¸‹è½½] å“åº”çŠ¶æ€: {status}, Content-Type: {content_type}")
                        
                        if status not in (200, 201, 202, 203, 206):
                            logger.warning(f"[PDFä¸‹è½½] éæˆåŠŸçŠ¶æ€ç  {status}ï¼Œç­‰å¾…åé‡è¯•...")
                            await asyncio.sleep(2 ** attempt)
                            continue

                        if 'application/pdf' not in content_type:
                             logger.warning(f"[PDFä¸‹è½½] Content-Type ä¸º {content_type}ï¼Œå¯èƒ½ä¸æ˜¯ PDF æ–‡ä»¶ï¼Œç»§ç»­å°è¯•ä¸‹è½½...")

                        total_size = 0
                        with open(pdf_path, 'wb') as f:
                            async for chunk in resp.content.iter_chunked(1024 * 1024):
                                if chunk:
                                    f.write(chunk)
                                    total_size += len(chunk)
                        
                        logger.info(f"[PDFä¸‹è½½] æ–‡ä»¶å·²ä¿å­˜: {pdf_path}, å¤§å°: {total_size} å­—èŠ‚")
                        
                        text_content = await self._process_pdf_in_thread(pdf_path, img_path)
                        
                        if os.path.exists(pdf_path):
                            try:
                                os.remove(pdf_path)
                            except Exception:
                                pass
                        
                        if not text_content:
                            logger.error(f"[PDFä¸‹è½½] PDFè§£æè¿”å›ç©ºæ–‡æœ¬")
                            if os.path.exists(img_path):
                                try:
                                    os.remove(img_path)
                                except Exception:
                                    pass
                            await asyncio.sleep(2)
                            continue
                        
                        logger.info(f"[PDFä¸‹è½½] æˆåŠŸï¼")
                        return text_content, img_path
                        
            except aiohttp.ClientError as e:
                logger.warning(f"[PDFä¸‹è½½] ç½‘ç»œå®¢æˆ·ç«¯é”™è¯¯ (å°è¯• {attempt+1}): {e}")
            except asyncio.TimeoutError:
                logger.warning(f"[PDFä¸‹è½½] è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt+1})")
            except Exception as e:
                logger.error(f"[PDFä¸‹è½½] æœªçŸ¥é”™è¯¯ (å°è¯• {attempt+1}): {e}", exc_info=True)
            
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)
        
        logger.error(f"[PDFä¸‹è½½] å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {max_retries}")
        return None, None

    async def translate_title(self, title):
        provider = self.context.get_using_provider()
        if not provider:
            return title
        
        try:
            prompt = f"Please translate the following scientific paper title into Chinese. Only output the translated title, do not output anything else.\n\nTitle: {title}"
            response = await provider.text_chat(prompt=prompt)
            cn_title = response.completion_text.strip().strip('"').strip("'")
            return cn_title
        except Exception as e:
            logger.warning(f"Title translation failed: {e}")
            return title

    async def get_ai_summary(self, title, abstract, full_text, authors):
        provider = self.context.get_using_provider()
        if not provider:
            return "é”™è¯¯ï¼šæœªé…ç½® AI æ¨¡å‹ã€‚"
            
        full_text_snippet = full_text[:3000]
        authors_str = ", ".join(authors)
        
        prompt = self.prompt_template.format(
            title=title,
            abstract=abstract,
            full_text=full_text_snippet,
            authors=authors_str
        )
        
        try:
            response = await provider.text_chat(prompt=prompt)
            return response.completion_text
        except Exception as e:
            return f"AI è§£è¯»ç”Ÿæˆå¤±è´¥: {e}"

    async def _broadcast_to_groups(self, message_chain: MessageChain):
        """ä½¿ç”¨é€šç”¨æ¥å£å¹¿æ’­åˆ°é…ç½®çš„ç¾¤ç»„"""
        if not self.target_groups:
            return
        
        platforms = self.context.platform_manager.get_insts()
        adapter = next((p for p in platforms if isinstance(p, AiocqhttpAdapter)), None)
        
        if not adapter:
            logger.error("æœªæ‰¾åˆ° aiocqhttp (QQ) é€‚é…å™¨ï¼Œæ— æ³•å‘é€ç¾¤æ¶ˆæ¯ã€‚")
            return

        for group_id in self.target_groups:
            try:
                logger.info(f"æ­£åœ¨å‘é€åˆ°ç¾¤: {group_id}")
                await AiocqhttpMessageEvent.send_message(
                    bot=adapter.bot,
                    message_chain=message_chain,
                    is_group=True,
                    session_id=group_id
                )
                await asyncio.sleep(2) 
            except Exception as e:
                logger.error(f"å‘é€åˆ°ç¾¤ {group_id} å¤±è´¥: {e}")

    async def _execute_push(self, paper, target_umo=None, is_manual=False, silent_start=False):
        """æ‰§è¡Œæ¨é€é€»è¾‘"""
        # å˜é‡åˆå§‹åŒ–
        raw_text = None
        img_path = None
        cn_title = None
        display_title = None
        explanation = None
        
        try:
            # 1. å‘é€æç¤º
            if not silent_start and (is_manual or self.target_groups):
                start_msg = MessageChain([Plain(f"ğŸ“„ æ­£åœ¨è·å–è®ºæ–‡: {paper['title']} ...")])
                if is_manual and target_umo:
                    await self.context.send_message(target_umo, start_msg)
                elif not is_manual:
                    await self._broadcast_to_groups(start_msg)
            else:
                logger.info(f"æ­£åœ¨åå°å¤„ç†è®ºæ–‡: {paper['title']} ...")

            # 2. å¤„ç†å†…å®¹
            pdf_task = self.process_pdf(paper['pdf_link'], paper['id'])
            trans_task = self.translate_title(paper['title'])
            
            results = await asyncio.gather(pdf_task, trans_task)
            (raw_text, img_path), cn_title = results
            
            if not raw_text or not img_path:
                err_msg = MessageChain([Plain("âš ï¸ PDF ä¸‹è½½æˆ–è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—è·å–è¯¦æƒ…ã€‚")])
                if is_manual and target_umo:
                    await self.context.send_message(target_umo, err_msg)
                elif not is_manual:
                    logger.error("PDF å¤„ç†å¤±è´¥ï¼Œå·²åœæ­¢æ¨é€")
                return

            # 3. ç”Ÿæˆæ€»ç»“
            explanation = await self.get_ai_summary(paper['title'], paper['summary'], raw_text, paper['authors'])
            
            # 4. å‡†å¤‡å‘é€å†…å®¹
            try:
                self_uin = self.context.platform_manager.get_insts()[0].client_self_id
            except IndexError:
                self_uin = "10000" 

            display_title = f"{cn_title}\n{paper['title']}"

            node1_content: list[BaseMessageComponent] = [
                Plain(f"ğŸ“„ æ ‡é¢˜:\n{display_title}\n\n"),
                Plain(f"ğŸ‘¥ ä½œè€…: {', '.join(paper['authors'][:3])} et al.\n"),
                Plain(f"ğŸ”— é“¾æ¥: {paper['link']}\n"),
                Image.fromFileSystem(img_path)
            ]
            node1 = Node(name="è®ºæ–‡é¢„è§ˆ", uin=self_uin, content=node1_content)
            
            node2_content: list[BaseMessageComponent] = [
                Plain(f"è§£è¯»ä¸€ä¸‹~\n\n{explanation}")
            ]
            node2 = Node(name="AI åŠ©æ‰‹", uin=self_uin, content=node2_content)
            
            all_nodes = [node1, node2]

            if self.extra_message and self.extra_message.strip():
                node3_content: list[BaseMessageComponent] = [Plain(self.extra_message)]
                node3 = Node(name="è¡¥å……ä¿¡æ¯", uin=self_uin, content=node3_content)
                all_nodes.append(node3)
            
            nodes_component = Nodes(all_nodes)
            forward_msg = MessageChain([nodes_component])
            end_msg = MessageChain([Plain("ä»Šæ—¥ AI è®ºæ–‡å·²é€è¾¾~")])
            
            # 5. å‘é€æ¶ˆæ¯
            if is_manual and target_umo:
                # æ‰‹åŠ¨è§¦å‘
                await self.context.send_message(target_umo, forward_msg)
            elif not is_manual:
                # å®šæ—¶ä»»åŠ¡å¹¿æ’­
                await self._broadcast_to_groups(forward_msg)
                await asyncio.sleep(2)
                await self._broadcast_to_groups(end_msg)
            
            # 6. è®°å½•å†å²
            if not is_manual:
                self.history.append(paper['id'])
                self._save_history()
            
            logger.info(f"è®ºæ–‡ {paper['title']} æ¨é€æˆåŠŸ")
                
        except Exception as e:
            logger.error(f"æ¶ˆæ¯å‘é€å¤±è´¥: {e}")
            # é™çº§: æ‹†å¼€å‘é€
            if display_title and img_path and explanation:
                try:
                    fallback_chain = MessageChain([
                        Plain(f"ğŸ“„ {display_title}\n{paper['link']}\n\n"),
                        Image.fromFileSystem(img_path),
                        Plain(f"\n\n{explanation}")
                    ])
                    if self.extra_message:
                        fallback_chain.chain.append(Plain(f"\n\n{self.extra_message}"))

                    if is_manual and target_umo:
                        await self.context.send_message(target_umo, fallback_chain)
                    elif not is_manual:
                        await self._broadcast_to_groups(fallback_chain)
                except Exception as e2:
                    logger.error(f"é™çº§å‘é€å¤±è´¥: {e2}")
            else:
                logger.error("å…³é”®æ•°æ®ç¼ºå¤±ï¼Œæ— æ³•æ‰§è¡Œé™çº§å‘é€ã€‚")

        finally:
            # 7. æ¸…ç†ä¸´æ—¶å›¾ç‰‡
            if img_path and os.path.exists(img_path):
                try:
                    os.remove(img_path)
                except Exception as e:
                    logger.warning(f"æ¸…ç†å›¾ç‰‡å¤±è´¥: {e}")

    async def run_daily_push(self):
        """æ¯æ—¥å®šæ—¶ä»»åŠ¡"""
        logger.info(">>> å¼€å§‹æ‰§è¡Œæ¯æ—¥è®ºæ–‡æ¨é€ä»»åŠ¡")
        
        if not self.target_groups:
            logger.warning("æœªé…ç½®æ¨é€ç›®æ ‡ç¾¤ (target_groups)ï¼Œä»»åŠ¡è·³è¿‡ã€‚")
            return
            
        try:
            paper = await self.fetch_latest_paper()
            if not paper:
                logger.warning("æœªè·å–åˆ°æ–°è®ºæ–‡")
                return
            
            await self._execute_push(paper, is_manual=False, silent_start=True)
            
        except Exception as e:
            logger.error(f"å®šæ—¶æ¨é€ä»»åŠ¡å¼‚å¸¸: {e}")

    # --- æŒ‡ä»¤éƒ¨åˆ† ---

    @filter.command("paper_set_group")
    @filter.permission_type(filter.PermissionType.ADMIN)
    async def set_group(self, event: AstrMessageEvent):
        """æ·»åŠ å½“å‰ç¾¤åˆ°æ¨é€åˆ—è¡¨"""
        group_id = event.get_group_id()
        if not group_id:
            yield event.plain_result("è¯·åœ¨ç¾¤èŠä¸­ä½¿ç”¨æ­¤æŒ‡ä»¤ã€‚")
            return

        if group_id in self.target_groups:
             yield event.plain_result("å½“å‰ç¾¤èŠå·²åœ¨æ¨é€åˆ—è¡¨ä¸­ã€‚")
             return

        self.target_groups.append(group_id)
        
        plugin_md = self.context.get_registered_star("astrbot_plugin_daily_paper")
        if plugin_md and plugin_md.config:
             plugin_md.config["target_groups"] = ",".join(self.target_groups)
             plugin_md.config.save_config()
        
        yield event.plain_result(f"âœ… å·²æ·»åŠ ç¾¤ {group_id} åˆ°æ¯æ—¥æ¨é€åˆ—è¡¨ã€‚")

    @filter.command("paper_push")
    @filter.permission_type(filter.PermissionType.ADMIN)
    async def push_specific(self, event: AstrMessageEvent, query: str):
        """æ¨é€æŒ‡å®šè®ºæ–‡"""
        if not query:
            yield event.plain_result("è¯·è¾“å…¥è®ºæ–‡é“¾æ¥ã€æ ‡é¢˜æˆ– IDã€‚")
            return

        yield event.plain_result("ğŸ” æ­£åœ¨ ArXiv æ£€ç´¢è®ºæ–‡ä¿¡æ¯...")
        paper = await self.fetch_specific_paper(query)
        
        if not paper:
            yield event.plain_result("âŒ æœªåœ¨ ArXiv ä¸Šæ‰¾åˆ°ç›¸å…³è®ºæ–‡ï¼Œè¯·æ£€æŸ¥è¾“å…¥ã€‚")
            return
            
        target = event.unified_msg_origin
        # æ‰‹åŠ¨æŒ‡ä»¤ï¼šsilent_start=False (é»˜è®¤)ï¼Œä¼šæ˜¾ç¤ºæ­£åœ¨è·å–
        await self._execute_push(paper, target_umo=target, is_manual=True)

    @filter.command("paper_push_now")
    @filter.permission_type(filter.PermissionType.ADMIN)
    async def push_now(self, event: AstrMessageEvent):
        """ç«‹å³è§¦å‘è‡ªåŠ¨æ¨é€"""
        paper = await self.fetch_latest_paper()
        if not paper:
            yield event.plain_result("æ²¡æœ‰è·å–åˆ°æ–°çš„å¾…æ¨é€è®ºæ–‡ã€‚")
            return
            
        await self._execute_push(paper, target_umo=event.unified_msg_origin, is_manual=True, silent_start=False)
